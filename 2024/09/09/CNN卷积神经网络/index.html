<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>CNN卷积神经网络 | 久书的Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  
  
    <link rel="alternate" href="/atom.xml" title="久书的Blog" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
    
<link rel="stylesheet" href="/localshare/css/share.css">

  
  
  
<meta name="generator" content="Hexo 7.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">久书的Blog</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        
          <a class="main-nav-link" href="/."><i class="fa fa-home"></i> Home</a>
        
          <a class="main-nav-link" href="/archives/"><i class="fa fa-archive"></i> Archive</a>
        
          <a class="main-nav-link" href="/about/"><i class="fa fa-user"></i> About</a>
        
          <a class="main-nav-link" href="/atom.xml"><i class="fa fa-rss"></i> RSS</a>
        
      </nav>
    </div>
    <div id="search-form">
      <div id="result-mask" class="hide"></div>
      <label><input id="search-key" type="text" autocomplete="off" placeholder="search"></label>
      <div id="result-wrap" class="hide">
        <div id="search-result"></div>
      </div>
      <div class="hide">
        <template id="search-tpl">
          <div class="item">
            <a href="/{path}" title="{title}">
              <div class="title">{title}</div>
              <div class="time">{date}</div>
              <div class="tags">{tags}</div>
            </a>
          </div>
        </template>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-CNN卷积神经网络" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      CNN卷积神经网络
    </h1>
  


      </header>
    
    <div class="article-meta">
      
      <span class="article-date">
  <i class="fa fa-date"></i>
  <time class="dt-published" datetime="2024-09-09T11:06:35.000Z" itemprop="datePublished">2024年09月09日</time>
</span>
      
      
        <span class="article-views">
  <i class="fa fa-views"></i>
  <i id="busuanzi_container_page_pv">
      <i id="busuanzi_value_page_pv"></i>
  </i>
</span>

      
      
<a href="/2024/09/09/CNN%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/#comments" class="article-comment-link">
  
    
      <span class="post-comments-count valine-comment-count" data-xid="/2024/09/09/CNN%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" itemprop="commentCount"></span>
    
    
    
    
    
  
  <i class="fa fa-commt"></i>
  Guestbook
</a>


    </div>
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="CNN卷积神经网络"><a href="#CNN卷积神经网络" class="headerlink" title="CNN卷积神经网络"></a>CNN卷积神经网络</h1><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p><code>卷积神经网络（Convolutional Neural Network，CNN）</code>是一种在计算机视觉领域取得了巨大成功的深度学习模型。它们的设计灵感来自于生物学中的视觉系统，旨在模拟人类视觉处理的方式。在过去的几年中，CNN已经在图像识别、目标检测、图像生成和许多其他领域取得了显著的进展，成为了计算机视觉和深度学习研究的重要组成部分。</p>
<h2 id="什么是卷积"><a href="#什么是卷积" class="headerlink" title="什么是卷积"></a>什么是卷积</h2><p>在卷积神经网络中，卷积操作是指将一个可移动的<strong>小窗口（称为数据窗口，如下图绿色矩形）</strong>与图像进行逐元素相乘然后相加的操作。这个小窗口其实是一组固定的权重，它可以被看作是一个特定的<strong>滤波器（filter）</strong>或<strong>卷积核</strong>。这个操作的名称“卷积”，源自于这种元素级相乘和求和的过程。这一操作是卷积神经网络名字的来源。</p>
<img src="7cabb107dc4fd77c9edd4fe790bfea17.png" alt="img">

<p>上图这个绿色小窗就是数据窗口。简而言之，<strong>卷积操作就是用一个可移动的小窗口来提取图像中的特征</strong>，这个小窗口包含了一组特定的权重，通过与图像的不同位置进行卷积操作，网络能够学习并捕捉到不同特征的信息。</p>
<p>下图中蓝色的框就是指一个数据窗口，红色框为卷积核（滤波器），最后得到的绿色方形就是卷积的结果（数据窗口中的数据与卷积核逐个元素相乘再求和）</p>
<img src="d0172774f7e42ae2f6310b63e59b4906.gif" alt="img">

<h2 id="卷积神经网络层级结构"><a href="#卷积神经网络层级结构" class="headerlink" title="卷积神经网络层级结构"></a>卷积神经网络层级结构</h2><p>一个卷积神经网络主要由以下5层组成：</p>
<ul>
<li>数据输入层&#x2F; Input layer</li>
<li>卷积计算层&#x2F; CONV layer</li>
<li>ReLU激励层</li>
<li>池化层 &#x2F; Pooling layer</li>
<li>全连接层 &#x2F; FC layer</li>
</ul>
<img src="3c266da23107494b04b09683b8427f0e.png" alt="img">

<h3 id="数据输入层"><a href="#数据输入层" class="headerlink" title="数据输入层"></a>数据输入层</h3><p>该层要做的处理主要是对原始图像数据进行预处理，其中包括：</p>
<ul>
<li><strong>去均值</strong>：把输入数据各个维度都中心化为0，如下图所示，其目的就是把样本的中心拉回到坐标系原点上。</li>
<li><strong>归一化</strong>：幅度归一化到同样的范围，如下所示，即减少各维度数据取值范围的差异而带来的干扰，比如，我们有两个维度的特征A和B，A范围是0到10，而B范围是0到10000，如果直接使用这两个特征是有问题的，好的做法就是归一化，即A和B的数据都变为0到1的范围。</li>
<li><strong>PCA&#x2F;白化</strong>：用PCA降维；白化是对数据各个特征轴上的幅度归一化</li>
</ul>
<p>去均值与归一化效果图：</p>
<img src="v2-4c3b00f07cce0c7dccf2e4ba5e167e30_r.jpg" alt="img">

<p>去相关与白化效果图：</p>
<img src="v2-229a2c9828a26d594dc854b659cfc8a5_1440w.webp" alt="img">

<h3 id="卷积计算层"><a href="#卷积计算层" class="headerlink" title="卷积计算层"></a>卷积计算层</h3><p>这一层就是卷积神经网络最重要的一个层次，也是“卷积神经网络”的名字来源。<br>在这个卷积层，有两个关键操作：</p>
<ul>
<li><strong>局部关联</strong>。每个神经元看做一个滤波器(filter)</li>
<li><strong>窗口(receptive field)滑动</strong>， filter对局部数据计算</li>
</ul>
<p>先介绍卷积层遇到的几个名词：</p>
<ul>
<li><strong>深度&#x2F;depth</strong>：卷积核的个数</li>
<li><strong>步幅&#x2F;stride</strong>：窗口一次滑动的长度</li>
<li><strong>填充值&#x2F;zero-padding</strong>：在外围边缘补充若干圈0，方便从初始位置以步长为单位可以刚好滑倒末尾位置，通俗地讲就是为了总长能被步长整除。</li>
</ul>
<img src="v2-821048bfaee14d8c03cc5044e04fe336_r.jpg" alt="img">

<h4 id="为什么要填充"><a href="#为什么要填充" class="headerlink" title="为什么要填充"></a>为什么要填充</h4><ol>
<li><strong>保留边缘信息</strong>：当卷积核在图像边缘进行卷积时，如果不进行填充，边缘的像素将无法得到完整的卷积运算，因为卷积核无法完全覆盖边缘区域。通过填充，可以确保边缘像素也能被卷积核完整覆盖，从而保留更多的图像信息。</li>
<li><strong>控制输出尺寸</strong>：填充可以控制卷积层输出的特征图（Feature Map）的尺寸。例如，通过适当的填充，可以使输出特征图的尺寸与输入图像的尺寸相同，这在某些网络结构设计中是必要的。</li>
<li><strong>防止信息丢失</strong>：在多次卷积操作中，如果不使用填充，每次卷积都会导致输出尺寸减小，这可能会导致图像的边缘信息快速丢失。适当的填充可以减缓尺寸的减小，避免信息丢失过快。</li>
<li><strong>实现特定的网络结构</strong>：在某些网络结构中，为了保持输入和输出的形状匹配，或者为了实现特定的网络功能，可能需要使用填充。</li>
</ol>
<p>如果你的步幅为 1，而且把零填充设置为</p>
<img src="v2-96a40f41090b4bb5e7ebbf0a0e186d9a_1440w.webp" alt="img">

<p>K 是过滤器尺寸，那么输入和输出内容就总能保持一致的空间维度。</p>
<p>计算任意给定卷积层的输出的大小的公式是</p>
<img src="v2-2ea24e54873121ae877b5e7f03db8844_1440w.webp" alt="img">

<p>其中 O 是输出尺寸，K 是过滤器尺寸，P 是填充，S 是步幅。</p>
<h3 id="激活层（非线性层）"><a href="#激活层（非线性层）" class="headerlink" title="激活层（非线性层）"></a>激活层（非线性层）</h3><p>把卷积层输出结果做非线性映射</p>
<img src="v2-4f12096f7b6fb83ce6dc96b3ecf915c8_1440w.webp" alt="img">



<p>CNN采用的激活函数一般为ReLU(The Rectified Linear Unit&#x2F;修正线性单元)，它的特点是收敛快，求梯度简单，但较脆弱，图像如下。</p>
<img src="v2-a559927aa4df378c6b1a25c2cb86db5b_1440w.webp" alt="img">

<p><strong>激励层的实践经验：</strong></p>
<p>①不要用sigmoid！不要用sigmoid！不要用sigmoid<br>② 首先试RELU，因为快，但要小心点<br>③ 如果2失效，请用Leaky ReLU或者Maxout<br>④ 某些情况下tanh倒是有不错的结果，但是很少</p>
<h3 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h3><p>池化层夹在连续的卷积层中间， 用于压缩数据和参数的量，减小<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?q=%E8%BF%87%E6%8B%9F%E5%90%88&zhida_source=entity&is_preview=1">过拟合</a>。<br>简而言之，如<strong>果输入是图像的话，那么池化层的最主要作用就是压缩图像。</strong></p>
<p>这里再展开叙述池化层的具体作用：</p>
<ol>
<li>**特征<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?q=%E4%B8%8D%E5%8F%98%E6%80%A7&zhida_source=entity&is_preview=1">不变性</a>**，也就是我们在图像处理中经常提到的特征的尺度不变性，池化操作就是图像的resize，平时一张狗的图像被缩小了一倍我们还能认出这是一张狗的照片，这说明这张图像中仍保留着狗最重要的特征，我们一看就能判断图像中画的是一只狗，<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?q=%E5%9B%BE%E5%83%8F%E5%8E%8B%E7%BC%A9&zhida_source=entity&is_preview=1">图像压缩</a>时去掉的信息只是一些无关紧要的信息，而留下的信息则是具有尺度不变性的特征，是最能表达图像的特征。</li>
<li>**<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?q=%E7%89%B9%E5%BE%81%E9%99%8D%E7%BB%B4&zhida_source=entity&is_preview=1">特征降维</a>**，我们知道一幅图像含有的信息是很大的，特征也很多，但是有些信息对于我们做图像任务时没有太多用途或者有重复，我们可以把这类<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?q=%E5%86%97%E4%BD%99%E4%BF%A1%E6%81%AF&zhida_source=entity&is_preview=1">冗余信息</a>去除，把最重要的特征抽取出来，这也是池化操作的一大作用。</li>
<li>在一定程度上<strong>防止过拟合</strong>，更方便优化。</li>
</ol>
<img src="v2-deeacf1fc2ef42c0e41070fae4fb5381_1440w-1725865814124-32-1725865824045-34.webp" alt="img">



<p>池化层用的方法有Max pooling 和 average pooling，而实际用的较多的是<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?q=Max+pooling&zhida_source=entity&is_preview=1">Max pooling</a>。这里就说一下Max pooling，其实思想非常简单。</p>
<img src="v2-7b28abd70e3bc4294b2b28cc6ff348ef_1440w.webp" alt="img">



<p>对于每个2 * 2的窗口选出最大的数作为输出矩阵的相应元素的值，比如输入矩阵第一个2 * 2窗口中最大的数是6，那么输出矩阵的第一个元素就是6，如此类推。</p>
<h3 id="全连接层"><a href="#全连接层" class="headerlink" title="全连接层"></a>全连接层</h3><p>两层之间所有神经元都有权重连接，通常全连接层在卷积神经网络尾部。也就是跟传统的<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?q=%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%A5%9E%E7%BB%8F%E5%85%83&zhida_source=entity&is_preview=1">神经网络神经元</a>的连接方式是一样的：</p>
<img src="v2-9cbccaabf38a4c5c4c8494afc3556c12_1440w.webp" alt="img">



<h2 id="CNN代码实现一般步骤"><a href="#CNN代码实现一般步骤" class="headerlink" title="CNN代码实现一般步骤"></a>CNN代码实现一般步骤</h2><p>在实现卷积神经网络（CNN）时，常见的步骤可以分为以下几个部分。下面我将以 Python 和 TensorFlow&#x2F;Keras 为例，介绍 CNN 的一般实现步骤：</p>
<h3 id="1-导入必要的库"><a href="#1-导入必要的库" class="headerlink" title="1. 导入必要的库"></a>1. <strong>导入必要的库</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> datasets, layers, models</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure>

<h3 id="2-加载和预处理数据"><a href="#2-加载和预处理数据" class="headerlink" title="2. 加载和预处理数据"></a>2. <strong>加载和预处理数据</strong></h3><p>以 CIFAR-10 数据集为例，加载并预处理数据（包括归一化、划分训练集和测试集等）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加载CIFAR-10数据集</span></span><br><span class="line">(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据归一化到0-1之间</span></span><br><span class="line">train_images, test_images = train_images / <span class="number">255.0</span>, test_images / <span class="number">255.0</span></span><br></pre></td></tr></table></figure>

<h3 id="3-构建CNN模型"><a href="#3-构建CNN模型" class="headerlink" title="3. 构建CNN模型"></a>3. <strong>构建CNN模型</strong></h3><p>构建一个简单的卷积神经网络模型，包括卷积层（Conv2D）、池化层（MaxPooling2D）、全连接层（Dense）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">model = models.Sequential()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第1层卷积层和池化层</span></span><br><span class="line">model.add(layers.Conv2D(<span class="number">32</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">&#x27;relu&#x27;</span>, input_shape=(<span class="number">32</span>, <span class="number">32</span>, <span class="number">3</span>)))</span><br><span class="line">model.add(layers.MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第2层卷积层和池化层</span></span><br><span class="line">model.add(layers.Conv2D(<span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">model.add(layers.MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第3层卷积层和池化层</span></span><br><span class="line">model.add(layers.Conv2D(<span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">model.add(layers.MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 展平（Flatten）层，把3D特征图转成1D</span></span><br><span class="line">model.add(layers.Flatten())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 全连接层</span></span><br><span class="line">model.add(layers.Dense(<span class="number">64</span>, activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出层，10个类</span></span><br><span class="line">model.add(layers.Dense(<span class="number">10</span>))</span><br></pre></td></tr></table></figure>

<h3 id="4-编译模型"><a href="#4-编译模型" class="headerlink" title="4. 编译模型"></a>4. <strong>编译模型</strong></h3><p>在编译步骤中，指定损失函数、优化器和评估指标。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model.<span class="built_in">compile</span>(optimizer=<span class="string">&#x27;adam&#x27;</span>,</span><br><span class="line">              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=<span class="literal">True</span>),</span><br><span class="line">              metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br></pre></td></tr></table></figure>

<h3 id="5-训练模型"><a href="#5-训练模型" class="headerlink" title="5. 训练模型"></a>5. <strong>训练模型</strong></h3><p>使用训练数据训练模型，设置训练的批次大小和训练的轮数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">history = model.fit(train_images, train_labels, epochs=<span class="number">10</span>, </span><br><span class="line">                    validation_data=(test_images, test_labels))</span><br></pre></td></tr></table></figure>

<h3 id="6-评估模型"><a href="#6-评估模型" class="headerlink" title="6. 评估模型"></a>6. <strong>评估模型</strong></h3><p>使用测试数据评估模型的准确性。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;\n测试准确率: <span class="subst">&#123;test_acc&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="7-可视化训练过程"><a href="#7-可视化训练过程" class="headerlink" title="7. 可视化训练过程"></a>7. <strong>可视化训练过程</strong></h3><p>可以使用 Matplotlib 来可视化训练过程中的准确率和损失变化。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(history.history[<span class="string">&#x27;accuracy&#x27;</span>], label=<span class="string">&#x27;训练准确率&#x27;</span>)</span><br><span class="line">plt.plot(history.history[<span class="string">&#x27;val_accuracy&#x27;</span>], label=<span class="string">&#x27;验证准确率&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Epoch&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Accuracy&#x27;</span>)</span><br><span class="line">plt.legend(loc=<span class="string">&#x27;lower right&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ol>
<li><strong>加载数据</strong>：准备数据并进行预处理。</li>
<li><strong>构建模型</strong>：搭建卷积层、池化层、全连接层等。</li>
<li><strong>编译模型</strong>：指定损失函数和优化器。</li>
<li><strong>训练模型</strong>：通过数据训练模型。</li>
<li><strong>评估模型</strong>：用测试集评估模型性能。</li>
<li><strong>可视化结果</strong>：查看训练过程中模型的表现。</li>
</ol>
<p>这就是实现卷积神经网络的一般步骤。可以根据需要增加模型的复杂度或改变超参数来优化模型。</p>
<h2 id="1D-CNN"><a href="#1D-CNN" class="headerlink" title="1D-CNN"></a>1D-CNN</h2><p>1D卷积神经网络（1D-CNN）是一种处理一维序列数据的深度学习模型。它广泛应用于时间序列数据、自然语言处理、音频信号处理等领域。</p>
<h3 id="1-输入数据"><a href="#1-输入数据" class="headerlink" title="1. 输入数据"></a>1. <strong>输入数据</strong></h3><p>1D-CNN 的输入是一个一维的序列数据，通常形状为 <code>(batch_size, sequence_length, channels)</code>，其中：</p>
<ul>
<li><code>batch_size</code>：一次性输入的样本数。</li>
<li><code>sequence_length</code>：输入序列的长度，例如时间序列数据的时间步长或文本中的单词数量。</li>
<li><code>channels</code>：输入数据的维度，例如时间序列中可以是不同传感器的数据，文本中可以是词向量的维度。</li>
</ul>
<h3 id="2-卷积操作"><a href="#2-卷积操作" class="headerlink" title="2. 卷积操作"></a>2. <strong>卷积操作</strong></h3><p>1D卷积的本质是在一维数据上滑动卷积核（过滤器），并对局部区域进行特征提取。具体过程如下：</p>
<ul>
<li><strong>卷积核（kernel&#x2F;filter）</strong>：一个大小为 <code>k</code> 的窗口，会滑动经过输入序列，并在每次滑动时计算输入和卷积核的逐点乘积和。</li>
<li><strong>滑动窗口</strong>：卷积核从输入数据的一端开始，以设定的步长（stride）向右滑动，依次计算出一系列的卷积结果（称为特征图，feature map）。</li>
<li><strong>输出特征图</strong>：每个卷积核在输入数据上滑动并计算输出，结果是一个新的序列，称为特征图。多个卷积核可以提取不同的特征，产生多个特征图。</li>
</ul>
<p>公式上可以表示为：<br>$$<br>{output}[i] &#x3D; \sum_{j&#x3D;1}^{k} \text{input}[i+j] \times \text{kernel}[j]<br>$$<br>其中，<code>i</code> 表示卷积窗口的起始位置，<code>k</code> 是卷积核的大小。</p>
<h3 id="3-激活函数"><a href="#3-激活函数" class="headerlink" title="3. 激活函数"></a>3. <strong>激活函数</strong></h3><p>卷积后的特征图通常会经过一个非线性激活函数（例如 ReLU），来引入非线性特性，提高模型的表达能力。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">output = tf.nn.relu(conv_output)</span><br></pre></td></tr></table></figure>

<h3 id="4-池化操作（可选）"><a href="#4-池化操作（可选）" class="headerlink" title="4. 池化操作（可选）"></a>4. <strong>池化操作（可选）</strong></h3><p>1D-CNN 中通常使用池化操作（如最大池化 <code>max pooling</code> 或平均池化 <code>average pooling</code>），通过取窗口中的最大值或平均值来对特征进行下采样，减少数据维度，保留重要信息，同时防止过拟合。池化的窗口也是一维的，通常设定为固定大小。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pooled_output = tf.layers.max_pooling1d(output, pool_size=<span class="number">2</span>, strides=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>

<h3 id="5-多层卷积"><a href="#5-多层卷积" class="headerlink" title="5. 多层卷积"></a>5. <strong>多层卷积</strong></h3><p>1D-CNN 通常由多个卷积层堆叠而成，上一层的输出特征图作为下一层的输入，进一步提取特征。每一层的卷积核数量通常会增加，从而提取更多的特征。</p>
<h3 id="6-全连接层与分类层"><a href="#6-全连接层与分类层" class="headerlink" title="6. 全连接层与分类层"></a>6. <strong>全连接层与分类层</strong></h3><p>在卷积层的最后，1D-CNN 输出的特征图会通过一个全连接层（fully connected layer，FC）或多层感知器（MLP）进行进一步处理，通常用来进行分类、回归等任务。为了进行分类，最后的输出维度会设置为分类的类别数，并可能通过 softmax 函数将输出转化为概率分布。</p>
<h3 id="7-损失函数与优化"><a href="#7-损失函数与优化" class="headerlink" title="7. 损失函数与优化"></a>7. <strong>损失函数与优化</strong></h3><p>在分类任务中，1D-CNN 最终的输出会与真实标签计算损失（例如交叉熵损失），然后通过反向传播算法更新网络中的参数，优化模型的性能。</p>
<h3 id="1D-CNN-工作流程总结"><a href="#1D-CNN-工作流程总结" class="headerlink" title="1D-CNN 工作流程总结"></a>1D-CNN 工作流程总结</h3><ol>
<li><strong>输入</strong>：输入序列数据，如时间序列、文本数据等。</li>
<li><strong>卷积</strong>：卷积核滑动提取局部特征。</li>
<li><strong>激活</strong>：使用激活函数（如 ReLU）引入非线性。</li>
<li><strong>池化（可选）</strong>：下采样特征图，减少维度。</li>
<li><strong>多层卷积</strong>：重复堆叠卷积层，提取高级特征。</li>
<li><strong>全连接层</strong>：将提取的特征用于分类或回归任务。</li>
<li><strong>输出</strong>：分类或回归的结果，应用损失函数优化模型。</li>
</ol>
<h3 id="示例代码"><a href="#示例代码" class="headerlink" title="示例代码"></a>示例代码</h3><p>以下是一个简单的 1D-CNN 的实现示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义1D-CNN模型</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cnn_1d_model</span>(<span class="params">input_shape, num_classes</span>):</span><br><span class="line">    model = tf.keras.models.Sequential([</span><br><span class="line">        tf.keras.layers.Conv1D(filters=<span class="number">64</span>, kernel_size=<span class="number">3</span>, activation=<span class="string">&#x27;relu&#x27;</span>, input_shape=input_shape),</span><br><span class="line">        tf.keras.layers.MaxPooling1D(pool_size=<span class="number">2</span>),</span><br><span class="line">        tf.keras.layers.Conv1D(filters=<span class="number">128</span>, kernel_size=<span class="number">3</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">        tf.keras.layers.MaxPooling1D(pool_size=<span class="number">2</span>),</span><br><span class="line">        tf.keras.layers.Flatten(),</span><br><span class="line">        tf.keras.layers.Dense(<span class="number">64</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">        tf.keras.layers.Dense(num_classes, activation=<span class="string">&#x27;softmax&#x27;</span>)</span><br><span class="line">    ])</span><br><span class="line">    </span><br><span class="line">    model.<span class="built_in">compile</span>(optimizer=<span class="string">&#x27;adam&#x27;</span>, loss=<span class="string">&#x27;sparse_categorical_crossentropy&#x27;</span>, metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输入形状 (sequence_length, channels)，假设序列长度为100，特征维度为10</span></span><br><span class="line">input_shape = (<span class="number">100</span>, <span class="number">10</span>)</span><br><span class="line">num_classes = <span class="number">5</span></span><br><span class="line"></span><br><span class="line">model = cnn_1d_model(input_shape, num_classes)</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>

<h3 id="应用领域"><a href="#应用领域" class="headerlink" title="应用领域"></a>应用领域</h3><ul>
<li><strong>时间序列分析</strong>：如股票数据、传感器数据、心电图（ECG）信号。</li>
<li><strong>自然语言处理</strong>：文本分类、情感分析等任务。</li>
<li><strong>音频处理</strong>：如语音识别中的声学特征提取。</li>
</ul>
<p>1D-CNN 通过对一维序列数据的局部特征提取和特征组合，在许多应用中表现出色。</p>
<h2 id="2D-CNN"><a href="#2D-CNN" class="headerlink" title="2D-CNN"></a>2D-CNN</h2><p>2D卷积神经网络（2D-CNN）是处理二维数据的深度学习模型，特别适合处理图像、视频帧等二维结构的数据。它在计算机视觉任务（如图像分类、目标检测和分割）中广泛应用。以下是2D-CNN的详细过程：</p>
<h3 id="1-输入数据-1"><a href="#1-输入数据-1" class="headerlink" title="1. 输入数据"></a>1. <strong>输入数据</strong></h3><p>2D-CNN 的输入通常是二维数据，常见于图像。输入的形状一般为 <code>(batch_size, height, width, channels)</code>，其中：</p>
<ul>
<li><code>batch_size</code>：一次性处理的样本数。</li>
<li><code>height</code> 和 <code>width</code>：图像的高度和宽度。</li>
<li><code>channels</code>：通道数，如灰度图的通道数为1，RGB图像的通道数为3。</li>
</ul>
<p>例如，输入可能是 <code>28x28</code> 的灰度图像，形状为 <code>(batch_size, 28, 28, 1)</code>。</p>
<h3 id="2-卷积操作-1"><a href="#2-卷积操作-1" class="headerlink" title="2. 卷积操作"></a>2. <strong>卷积操作</strong></h3><p>卷积操作是 CNN 的核心，通过卷积核（filter）对输入图像进行局部特征提取。卷积操作通过将一个小窗口在输入图像上滑动，并计算其与窗口内数据的点积来生成输出特征图（feature map）。</p>
<h4 id="卷积步骤"><a href="#卷积步骤" class="headerlink" title="卷积步骤"></a><strong>卷积步骤</strong></h4><ul>
<li><strong>卷积核（kernel&#x2F;filter）</strong>：通常大小为 <code>k x k</code> 的二维矩阵（如 <code>3x3</code> 或 <code>5x5</code>），在输入图像上滑动。</li>
<li><strong>滑动窗口</strong>：卷积核从左上角开始，按照指定的步长（stride）在图像上滑动，生成每个局部区域的卷积输出。滑动的步长决定了特征图的输出大小。</li>
<li><strong>输出特征图</strong>：卷积操作在整个输入图像上滑动，输出一个新的二维特征图。多个卷积核可以提取不同的特征，因此会输出多个特征图。</li>
</ul>
<p>卷积操作的公式如下：<br>$$<br>{output}(i, j) &#x3D; \sum_{m&#x3D;1}^{k} \sum_{n&#x3D;1}^{k} \text{input}(i+m, j+n) \times \text{kernel}(m, n)<br>$$<br> 其中 <code>i, j</code> 是卷积核在输入图像上的位置。</p>
<h4 id="填充（Padding）"><a href="#填充（Padding）" class="headerlink" title="填充（Padding）"></a><strong>填充（Padding）</strong></h4><ul>
<li><strong>valid padding</strong>：不使用填充，卷积核只能滑动到图像的边界内，输出特征图的尺寸小于原图。</li>
<li><strong>same padding</strong>：在图像的边缘添加零填充，使得卷积操作后输出特征图的尺寸与输入图像相同。</li>
</ul>
<h3 id="3-激活函数-1"><a href="#3-激活函数-1" class="headerlink" title="3. 激活函数"></a>3. <strong>激活函数</strong></h3><p>卷积后的特征图通常通过激活函数（如 ReLU）来引入非线性，激活函数作用在每个卷积输出的元素上。ReLU（Rectified Linear Unit）的常用公式是：<br>$$<br>f(x)&#x3D;max⁡(0,x)<br>$$<br> ReLU 将负值置为 0，保持正值不变，提高了模型的非线性表达能力。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conv_output = tf.nn.relu(conv_output)</span><br></pre></td></tr></table></figure>

<h3 id="4-池化操作（Pooling）"><a href="#4-池化操作（Pooling）" class="headerlink" title="4. 池化操作（Pooling）"></a>4. <strong>池化操作（Pooling）</strong></h3><p>池化操作通过下采样的方式减少特征图的尺寸，从而减少模型计算量和防止过拟合。常见的池化方法有 <strong>最大池化（Max Pooling）</strong> 和 <strong>平均池化（Average Pooling）</strong>。</p>
<ul>
<li><strong>最大池化</strong>：取窗口内的最大值。</li>
<li><strong>平均池化</strong>：取窗口内所有值的平均值。</li>
</ul>
<p>池化窗口大小通常为 <code>2x2</code>，步长为2，这样每次池化会将特征图的大小减半。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pooled_output = tf.nn.max_pool(conv_output, ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">&#x27;SAME&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="5-多层卷积-1"><a href="#5-多层卷积-1" class="headerlink" title="5. 多层卷积"></a>5. <strong>多层卷积</strong></h3><p>2D-CNN 通常由多层卷积和池化层堆叠而成。每层卷积可以提取更高级的特征：</p>
<ul>
<li>第一层卷积可能提取边缘、角等低级特征。</li>
<li>随着层数加深，后续层能够提取更加复杂的特征，如物体的形状和结构。</li>
</ul>
<p>每一层卷积后的特征图通常会随着通道数增加，但空间维度（高度和宽度）通常会通过池化逐渐减小。</p>
<h3 id="6-全连接层（Fully-Connected-Layer-FC）"><a href="#6-全连接层（Fully-Connected-Layer-FC）" class="headerlink" title="6. 全连接层（Fully Connected Layer, FC）"></a>6. <strong>全连接层（Fully Connected Layer, FC）</strong></h3><p>在卷积层之后，通常会将2D特征图展平（Flatten），转化为一维向量，并通过全连接层进行分类或回归任务。</p>
<ul>
<li><strong>展平</strong>：将二维的特征图转换为一维向量，方便全连接层处理。</li>
<li><strong>全连接层</strong>：将展平的特征输入到全连接层，经过线性变换后输出最终的分类结果。最后一层的输出维度与类别数相等，表示每个类别的预测分数。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">flattened = tf.layers.flatten(pooled_output)</span><br><span class="line">fc = tf.layers.dense(flattened, units=<span class="number">128</span>, activation=tf.nn.relu)</span><br><span class="line">logits = tf.layers.dense(fc, units=num_classes)</span><br></pre></td></tr></table></figure>

<h3 id="7-损失函数与优化-1"><a href="#7-损失函数与优化-1" class="headerlink" title="7. 损失函数与优化"></a>7. <strong>损失函数与优化</strong></h3><p>对于分类任务，最后一层通常输出一个类别数的向量，通过 <strong>softmax</strong> 函数将其转化为概率分布：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predictions = tf.nn.softmax(logits)</span><br></pre></td></tr></table></figure>

<p>损失函数通常使用交叉熵损失，通过反向传播和优化器（如 Adam 或 SGD）来优化模型。</p>
<h3 id="2D-CNN-工作流程总结"><a href="#2D-CNN-工作流程总结" class="headerlink" title="2D-CNN 工作流程总结"></a>2D-CNN 工作流程总结</h3><ol>
<li><strong>输入</strong>：二维数据，如图像。</li>
<li><strong>卷积层</strong>：提取局部特征，卷积核在图像上滑动。</li>
<li><strong>激活函数</strong>：如 ReLU 引入非线性。</li>
<li><strong>池化层</strong>：下采样特征图，减少维度。</li>
<li><strong>多层卷积与池化</strong>：堆叠多个卷积和池化层提取高级特征。</li>
<li><strong>展平与全连接层</strong>：将特征图展平，输入全连接层。</li>
<li><strong>输出层</strong>：通过 softmax 等进行分类，输出类别概率。</li>
</ol>
<h3 id="示例代码-1"><a href="#示例代码-1" class="headerlink" title="示例代码"></a>示例代码</h3><p>以下是一个简单的2D-CNN实现示例，用于处理输入的图像数据：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义2D-CNN模型</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cnn_2d_model</span>(<span class="params">input_shape, num_classes</span>):</span><br><span class="line">    model = tf.keras.models.Sequential([</span><br><span class="line">        tf.keras.layers.Conv2D(filters=<span class="number">32</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">&#x27;relu&#x27;</span>, input_shape=input_shape),</span><br><span class="line">        tf.keras.layers.MaxPooling2D(pool_size=(<span class="number">2</span>, <span class="number">2</span>)),</span><br><span class="line">        tf.keras.layers.Conv2D(filters=<span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">        tf.keras.layers.MaxPooling2D(pool_size=(<span class="number">2</span>, <span class="number">2</span>)),</span><br><span class="line">        tf.keras.layers.Flatten(),</span><br><span class="line">        tf.keras.layers.Dense(<span class="number">128</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">        tf.keras.layers.Dense(num_classes, activation=<span class="string">&#x27;softmax&#x27;</span>)</span><br><span class="line">    ])</span><br><span class="line">    </span><br><span class="line">    model.<span class="built_in">compile</span>(optimizer=<span class="string">&#x27;adam&#x27;</span>, loss=<span class="string">&#x27;sparse_categorical_crossentropy&#x27;</span>, metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设输入形状为28x28的灰度图，通道数为1，类别数为10</span></span><br><span class="line">input_shape = (<span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>)</span><br><span class="line">num_classes = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">model = cnn_2d_model(input_shape, num_classes)</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>

<h3 id="应用领域-1"><a href="#应用领域-1" class="headerlink" title="应用领域"></a>应用领域</h3><ul>
<li><strong>图像分类</strong>：如手写数字识别（MNIST）、物体识别（CIFAR-10）。</li>
<li><strong>目标检测</strong>：如人脸检测、行人检测等。</li>
<li><strong>图像分割</strong>：如医学图像的病灶区域分割。</li>
</ul>
<p>2D-CNN 通过局部卷积提取图像中的层级特征，逐步缩小空间维度，保留重要的高层次语义信息，在图像处理中取得了显著的成功。</p>
<h2 id="3D-CNN"><a href="#3D-CNN" class="headerlink" title="3D-CNN"></a>3D-CNN</h2><p>3D卷积神经网络（3D-CNN）是处理三维数据的深度学习模型，常用于视频处理、医学图像（如CT、MRI扫描）和体素数据等领域。与2D-CNN不同，3D-CNN的卷积操作在三维空间中进行，能够捕捉数据的空间和时间上的特征。以下是3D-CNN的详细过程：</p>
<h3 id="1-输入数据-2"><a href="#1-输入数据-2" class="headerlink" title="1. 输入数据"></a>1. <strong>输入数据</strong></h3><p>3D-CNN的输入通常是三维数据，常见于视频或医学图像。输入的形状一般为 <code>(batch_size, depth, height, width, channels)</code>，其中：</p>
<ul>
<li><code>batch_size</code>：一次性处理的样本数。</li>
<li><code>depth</code>：深度，表示输入数据的第三个维度，例如视频的帧数、体素数据的深度等。</li>
<li><code>height</code> 和 <code>width</code>：高度和宽度，表示输入的二维空间维度。</li>
<li><code>channels</code>：通道数，表示输入数据的通道数（如RGB图像的通道数为3）。</li>
</ul>
<p>例如，输入可能是 <code>16x128x128</code> 的体积数据（深度16，宽度128，高度128），形状为 <code>(batch_size, 16, 128, 128, 1)</code>。</p>
<h3 id="2-卷积操作-2"><a href="#2-卷积操作-2" class="headerlink" title="2. 卷积操作"></a>2. <strong>卷积操作</strong></h3><p>3D卷积操作与2D卷积类似，不同之处在于它在三维空间中滑动卷积核，提取局部的体积特征。卷积核在<code>depth</code>、<code>height</code>、<code>width</code>三个维度上滑动，产生三维的特征图。</p>
<h4 id="卷积步骤-1"><a href="#卷积步骤-1" class="headerlink" title="卷积步骤"></a><strong>卷积步骤</strong></h4><ul>
<li><strong>卷积核（kernel&#x2F;filter）</strong>：大小为 <code>k x k x k</code> 的三维矩阵（例如 <code>3x3x3</code> 或 <code>5x5x5</code>），在输入的体积数据上滑动，生成输出特征图。</li>
<li><strong>滑动窗口</strong>：卷积核在三维输入数据上滑动，按照指定的步长（stride）进行移动，步长可以在三维空间内控制移动的步幅。</li>
<li><strong>输出特征图</strong>：每次滑动后，将局部的输入数据与卷积核计算点积，生成一个输出值。整个卷积过程结束后，会生成三维的特征图。</li>
</ul>
<p>公式上，3D卷积的操作可以表示为：<br>$$<br>{output}(i, j, k) &#x3D; \sum_{d&#x3D;1}^{D} \sum_{h&#x3D;1}^{H} \sum_{w&#x3D;1}^{W} \text{input}(i+d, j+h, k+w) \times \text{kernel}(d, h, w)<br>$$<br>其中，<code>i, j, k</code> 是卷积核在输入数据上的位置，<code>D, H, W</code> 分别是卷积核的深度、高度和宽度。</p>
<h4 id="填充（Padding）-1"><a href="#填充（Padding）-1" class="headerlink" title="填充（Padding）"></a><strong>填充（Padding）</strong></h4><ul>
<li><strong>valid padding</strong>：不使用填充，卷积核只能滑动到数据的边界内，输出的特征图比输入数据尺寸小。</li>
<li><strong>same padding</strong>：在输入数据的边缘添加零填充，确保输出特征图的尺寸与输入相同。</li>
</ul>
<h3 id="3-激活函数-2"><a href="#3-激活函数-2" class="headerlink" title="3. 激活函数"></a>3. <strong>激活函数</strong></h3><p>卷积后的特征图通常会通过一个激活函数（如 ReLU）引入非线性。ReLU（Rectified Linear Unit）是最常见的激活函数，它将负值置为 0，保持正值不变。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conv_output = tf.nn.relu(conv_output)</span><br></pre></td></tr></table></figure>

<h3 id="4-池化操作（Pooling）-1"><a href="#4-池化操作（Pooling）-1" class="headerlink" title="4. 池化操作（Pooling）"></a>4. <strong>池化操作（Pooling）</strong></h3><p>在3D-CNN中，池化操作通常也在三维空间内进行，称为 <strong>3D池化（3D pooling）</strong>。它通过对体积特征的下采样，减少数据的空间和深度维度，从而减少计算量和防止过拟合。常用的池化方法有：</p>
<ul>
<li><strong>最大池化（Max Pooling）</strong>：取池化窗口内的最大值。</li>
<li><strong>平均池化（Average Pooling）</strong>：取池化窗口内所有值的平均值。</li>
</ul>
<p>池化的窗口通常为 <code>2x2x2</code>，步长为2，这样每次池化操作会将特征图的大小减半。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pooled_output = tf.nn.max_pool3d(conv_output, ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">&#x27;SAME&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="5-多层卷积-2"><a href="#5-多层卷积-2" class="headerlink" title="5. 多层卷积"></a>5. <strong>多层卷积</strong></h3><p>3D-CNN通常由多层卷积和池化层堆叠而成。每一层卷积提取的特征越来越复杂，前面的卷积层可能提取边缘、角等低级特征，而后面的层则提取更高级的特征，如物体的形状或视频中的运动模式。</p>
<h3 id="6-全连接层（Fully-Connected-Layer-FC）-1"><a href="#6-全连接层（Fully-Connected-Layer-FC）-1" class="headerlink" title="6. 全连接层（Fully Connected Layer, FC）"></a>6. <strong>全连接层（Fully Connected Layer, FC）</strong></h3><p>在卷积层和池化层之后，输出的三维特征图会被展平（Flatten）为一维向量，并通过全连接层处理，用于分类或回归任务。</p>
<ul>
<li><strong>展平</strong>：将3D特征图转换为一维向量，方便后续的全连接层处理。</li>
<li><strong>全连接层</strong>：一维向量输入到全连接层，通过线性变换后输出分类结果。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">flattened = tf.layers.flatten(pooled_output)</span><br><span class="line">fc = tf.layers.dense(flattened, units=<span class="number">128</span>, activation=tf.nn.relu)</span><br><span class="line">logits = tf.layers.dense(fc, units=num_classes)</span><br></pre></td></tr></table></figure>

<h3 id="7-损失函数与优化-2"><a href="#7-损失函数与优化-2" class="headerlink" title="7. 损失函数与优化"></a>7. <strong>损失函数与优化</strong></h3><p>在分类任务中，最后的输出通常是一个类别数的向量，通过 <strong>softmax</strong> 函数将其转化为概率分布。损失函数通常使用交叉熵损失，然后通过反向传播和优化器（如Adam或SGD）来优化模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predictions = tf.nn.softmax(logits)</span><br></pre></td></tr></table></figure>

<h3 id="3D-CNN-工作流程总结"><a href="#3D-CNN-工作流程总结" class="headerlink" title="3D-CNN 工作流程总结"></a>3D-CNN 工作流程总结</h3><ol>
<li><strong>输入</strong>：三维数据，如视频序列、体素数据或医学图像（CT&#x2F;MRI）。</li>
<li><strong>卷积层</strong>：在三维数据上进行卷积操作，提取局部体积特征。</li>
<li><strong>激活函数</strong>：如 ReLU 引入非线性。</li>
<li><strong>池化层</strong>：对体积特征进行下采样，减少数据维度。</li>
<li><strong>多层卷积与池化</strong>：堆叠多个卷积层，提取更高级的特征。</li>
<li><strong>展平与全连接层</strong>：将三维特征图展平为一维向量，输入全连接层进行分类或回归。</li>
<li><strong>输出层</strong>：通过softmax等进行分类，输出类别概率。</li>
</ol>
<h3 id="示例代码-2"><a href="#示例代码-2" class="headerlink" title="示例代码"></a>示例代码</h3><p>以下是一个简单的3D-CNN实现示例，用于处理输入的三维体积数据（如视频或体素数据）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义3D-CNN模型</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cnn_3d_model</span>(<span class="params">input_shape, num_classes</span>):</span><br><span class="line">    model = tf.keras.models.Sequential([</span><br><span class="line">        tf.keras.layers.Conv3D(filters=<span class="number">32</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">&#x27;relu&#x27;</span>, input_shape=input_shape),</span><br><span class="line">        tf.keras.layers.MaxPooling3D(pool_size=(<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>)),</span><br><span class="line">        tf.keras.layers.Conv3D(filters=<span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">        tf.keras.layers.MaxPooling3D(pool_size=(<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>)),</span><br><span class="line">        tf.keras.layers.Flatten(),</span><br><span class="line">        tf.keras.layers.Dense(<span class="number">128</span>, activation=<span class="string">&#x27;relu&#x27;</span>),</span><br><span class="line">        tf.keras.layers.Dense(num_classes, activation=<span class="string">&#x27;softmax&#x27;</span>)</span><br><span class="line">    ])</span><br><span class="line">    </span><br><span class="line">    model.<span class="built_in">compile</span>(optimizer=<span class="string">&#x27;adam&#x27;</span>, loss=<span class="string">&#x27;sparse_categorical_crossentropy&#x27;</span>, metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设输入形状为16x128x128的体积数据，通道数为1，类别数为10</span></span><br><span class="line">input_shape = (<span class="number">16</span>, <span class="number">128</span>, <span class="number">128</span>, <span class="number">1</span>)</span><br><span class="line">num_classes = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">model = cnn_3d_model(input_shape, num_classes)</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>

<h3 id="应用领域-2"><a href="#应用领域-2" class="headerlink" title="应用领域"></a>应用领域</h3><ul>
<li><strong>视频分析</strong>：如动作识别、视频分类。</li>
<li><strong>医学图像处理</strong>：如三维CT、MRI扫描的病灶检测与分类。</li>
<li><strong>体素数据分析</strong>：如3D建模、3D物体识别。</li>
</ul>
<p>3D-CNN通过对三维数据的局部卷积提取体积特征，可以捕捉空间和时间上的依赖性，特别适合处理三维或多帧数据。</p>

        
            <div id="toc-article">
                
  <div class="widget-wrap" id="toc-wrap">
    <h3 class="widget-title"><i class="fa fa-toc"></i> Contents</h3>
    <div class="widget">
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#CNN%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-text">CNN卷积神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%95%E8%A8%80"><span class="toc-text">引言</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E5%8D%B7%E7%A7%AF"><span class="toc-text">什么是卷积</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%B1%82%E7%BA%A7%E7%BB%93%E6%9E%84"><span class="toc-text">卷积神经网络层级结构</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E8%BE%93%E5%85%A5%E5%B1%82"><span class="toc-text">数据输入层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E8%AE%A1%E7%AE%97%E5%B1%82"><span class="toc-text">卷积计算层</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E5%A1%AB%E5%85%85"><span class="toc-text">为什么要填充</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%BF%80%E6%B4%BB%E5%B1%82%EF%BC%88%E9%9D%9E%E7%BA%BF%E6%80%A7%E5%B1%82%EF%BC%89"><span class="toc-text">激活层（非线性层）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B1%A0%E5%8C%96%E5%B1%82"><span class="toc-text">池化层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82"><span class="toc-text">全连接层</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#CNN%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%E4%B8%80%E8%88%AC%E6%AD%A5%E9%AA%A4"><span class="toc-text">CNN代码实现一般步骤</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E5%AF%BC%E5%85%A5%E5%BF%85%E8%A6%81%E7%9A%84%E5%BA%93"><span class="toc-text">1. 导入必要的库</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E5%8A%A0%E8%BD%BD%E5%92%8C%E9%A2%84%E5%A4%84%E7%90%86%E6%95%B0%E6%8D%AE"><span class="toc-text">2. 加载和预处理数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E6%9E%84%E5%BB%BACNN%E6%A8%A1%E5%9E%8B"><span class="toc-text">3. 构建CNN模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E7%BC%96%E8%AF%91%E6%A8%A1%E5%9E%8B"><span class="toc-text">4. 编译模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B"><span class="toc-text">5. 训练模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-%E8%AF%84%E4%BC%B0%E6%A8%A1%E5%9E%8B"><span class="toc-text">6. 评估模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B"><span class="toc-text">7. 可视化训练过程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-text">总结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1D-CNN"><span class="toc-text">1D-CNN</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E8%BE%93%E5%85%A5%E6%95%B0%E6%8D%AE"><span class="toc-text">1. 输入数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E5%8D%B7%E7%A7%AF%E6%93%8D%E4%BD%9C"><span class="toc-text">2. 卷积操作</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-text">3. 激活函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E6%B1%A0%E5%8C%96%E6%93%8D%E4%BD%9C%EF%BC%88%E5%8F%AF%E9%80%89%EF%BC%89"><span class="toc-text">4. 池化操作（可选）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-%E5%A4%9A%E5%B1%82%E5%8D%B7%E7%A7%AF"><span class="toc-text">5. 多层卷积</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82%E4%B8%8E%E5%88%86%E7%B1%BB%E5%B1%82"><span class="toc-text">6. 全连接层与分类层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B8%8E%E4%BC%98%E5%8C%96"><span class="toc-text">7. 损失函数与优化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1D-CNN-%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B%E6%80%BB%E7%BB%93"><span class="toc-text">1D-CNN 工作流程总结</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B%E4%BB%A3%E7%A0%81"><span class="toc-text">示例代码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BA%94%E7%94%A8%E9%A2%86%E5%9F%9F"><span class="toc-text">应用领域</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2D-CNN"><span class="toc-text">2D-CNN</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E8%BE%93%E5%85%A5%E6%95%B0%E6%8D%AE-1"><span class="toc-text">1. 输入数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E5%8D%B7%E7%A7%AF%E6%93%8D%E4%BD%9C-1"><span class="toc-text">2. 卷积操作</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E6%AD%A5%E9%AA%A4"><span class="toc-text">卷积步骤</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A1%AB%E5%85%85%EF%BC%88Padding%EF%BC%89"><span class="toc-text">填充（Padding）</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0-1"><span class="toc-text">3. 激活函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E6%B1%A0%E5%8C%96%E6%93%8D%E4%BD%9C%EF%BC%88Pooling%EF%BC%89"><span class="toc-text">4. 池化操作（Pooling）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-%E5%A4%9A%E5%B1%82%E5%8D%B7%E7%A7%AF-1"><span class="toc-text">5. 多层卷积</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82%EF%BC%88Fully-Connected-Layer-FC%EF%BC%89"><span class="toc-text">6. 全连接层（Fully Connected Layer, FC）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B8%8E%E4%BC%98%E5%8C%96-1"><span class="toc-text">7. 损失函数与优化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2D-CNN-%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B%E6%80%BB%E7%BB%93"><span class="toc-text">2D-CNN 工作流程总结</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B%E4%BB%A3%E7%A0%81-1"><span class="toc-text">示例代码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BA%94%E7%94%A8%E9%A2%86%E5%9F%9F-1"><span class="toc-text">应用领域</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3D-CNN"><span class="toc-text">3D-CNN</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E8%BE%93%E5%85%A5%E6%95%B0%E6%8D%AE-2"><span class="toc-text">1. 输入数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E5%8D%B7%E7%A7%AF%E6%93%8D%E4%BD%9C-2"><span class="toc-text">2. 卷积操作</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E6%AD%A5%E9%AA%A4-1"><span class="toc-text">卷积步骤</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A1%AB%E5%85%85%EF%BC%88Padding%EF%BC%89-1"><span class="toc-text">填充（Padding）</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0-2"><span class="toc-text">3. 激活函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E6%B1%A0%E5%8C%96%E6%93%8D%E4%BD%9C%EF%BC%88Pooling%EF%BC%89-1"><span class="toc-text">4. 池化操作（Pooling）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-%E5%A4%9A%E5%B1%82%E5%8D%B7%E7%A7%AF-2"><span class="toc-text">5. 多层卷积</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82%EF%BC%88Fully-Connected-Layer-FC%EF%BC%89-1"><span class="toc-text">6. 全连接层（Fully Connected Layer, FC）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B8%8E%E4%BC%98%E5%8C%96-2"><span class="toc-text">7. 损失函数与优化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3D-CNN-%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B%E6%80%BB%E7%BB%93"><span class="toc-text">3D-CNN 工作流程总结</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B%E4%BB%A3%E7%A0%81-2"><span class="toc-text">示例代码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BA%94%E7%94%A8%E9%A2%86%E5%9F%9F-2"><span class="toc-text">应用领域</span></a></li></ol></li></ol></li></ol>
    </div>
  </div>


            </div>
        
        
          <blockquote id="copyright">
              <p>Original link: <a href="http://example.com/2024/09/09/CNN卷积神经网络/">http://example.com/2024/09/09/CNN卷积神经网络/</a></p>
              <p>Copyright Notice: 转载请注明出处.</p>
          </blockquote>
        
      
    </div>
    <footer class="article-footer">
      
        <div class="article-tag-wrap">
          

          
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/CNN-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag">CNN 深度学习</a></li></ul>

          
    <div class="social-share">
      <span>Share:</span>
    </div>



        </div>
      
      
        
<nav id="article-nav">
  
    <a href="/2024/09/09/test/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">older</strong>
      <div class="article-nav-title">
        
          test
        
      </div>
    </a>
  
  
    <a href="/2024/09/13/CNN%E4%BB%A3%E7%A0%81%E5%85%A5%E9%97%A8/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">newer</strong>
      <div class="article-nav-title">
        
          CNN代码入门
        
      </div>
    </a>
  
</nav>

      
      
        


  <section id="comments" class="vcomment"></section>







      
    </footer>
  </div>
</article>
</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title"><i class="fa fa-posts"></i> Recent</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2024/09/13/CNN%E4%BB%A3%E7%A0%81%E5%85%A5%E9%97%A8/">CNN代码入门</a>
          </li>
        
          <li>
            <a href="/2024/09/09/CNN%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">CNN卷积神经网络</a>
          </li>
        
          <li>
            <a href="/2024/09/09/test/">test</a>
          </li>
        
          <li>
            <a href="/2024/07/01/NumPy/">NumPy</a>
          </li>
        
          <li>
            <a href="/2024/06/30/hexo%E5%8D%9A%E5%AE%A2%E4%BD%BF%E7%94%A8/">hexo博客使用</a>
          </li>
        
      </ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title"><i class="fa fa-tag"></i> Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/CNN-%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/" style="font-size: 10px;">CNN 图像处理</a> <a href="/tags/CNN-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" style="font-size: 10px;">CNN 深度学习</a> <a href="/tags/NumPy/" style="font-size: 10px;">NumPy</a> <a href="/tags/git-%E5%AD%A6%E4%B9%A0/" style="font-size: 10px;">git 学习</a> <a href="/tags/python%E5%AD%A6%E4%B9%A0/" style="font-size: 10px;">python学习</a> <a href="/tags/%E5%8D%9A%E5%AE%A2%E4%BD%BF%E7%94%A8/" style="font-size: 10px;">博客使用</a> <a href="/tags/%E6%97%A5%E5%B8%B8%E7%A7%AF%E7%B4%AF/" style="font-size: 10px;">日常积累</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" style="font-size: 10px;">深度学习</a>
    </div>
  </div>

  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title"><i class="fa fa-archive"></i> Archive</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/">2024年09月</a><span class="archive-list-count">9</span></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title"><i class="fa fa-tag"></i> Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/CNN-%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/" rel="tag">CNN 图像处理</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CNN-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag">CNN 深度学习</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NumPy/" rel="tag">NumPy</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/git-%E5%AD%A6%E4%B9%A0/" rel="tag">git 学习</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python%E5%AD%A6%E4%B9%A0/" rel="tag">python学习</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%8D%9A%E5%AE%A2%E4%BD%BF%E7%94%A8/" rel="tag">博客使用</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%97%A5%E5%B8%B8%E7%A7%AF%E7%B4%AF/" rel="tag">日常积累</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag">深度学习</a><span class="tag-list-count">1</span></li></ul>
    </div>
  </div>


  
    

  
</aside>
        
      </div>
      <a id="totop" href="#top"></a>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      <p>
        <a href="/sitemap.xml">Site Map</a>
        <span> | </span><a href="/atom.xml">Subscribe to this site</a>
        <span> | </span><a href="/about/">Contact the blogger</a>
      </p>
      
        <p>
          <i class="fa fa-visitors"></i>
          <i id="busuanzi_container_site_uv"><i id="busuanzi_value_site_uv"></i></i>
          ，
          <i class="fa fa-views"></i>
          <i id="busuanzi_container_site_pv"><i id="busuanzi_value_site_pv"></i></i>
        </p>
      
      <p>
        <span>Copyright &copy; 2024 John Doe.</span>
        <span>Theme by <a href="https://github.com/chaooo/hexo-theme-BlueLake/" target="_blank">BlueLake.</a></span>
        <span>Powered by <a href="https://hexo.io/" target="_blank">Hexo.</a></span>
      </p>
    </div>
  </div>
</footer>

    </div>
  </div>
  
<script src="/js/jquery-3.4.1.min.js"></script>


<script src="/js/search.json.js"></script>


  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>






  
<script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  
    
<script src="/localshare/js/social-share.js"></script>

    
<script src="/localshare/js/qrcode.js"></script>

  
  



  

  

  
    
<script src="https://cdn.jsdelivr.net/npm/valine@1.3.10/dist/Valine.min.js"></script>

    <script>
      var GUEST_INFO = ['nick','mail','link'];
      var guest_info = 'nick,mail,link'.split(',').filter(function(item){
          return GUEST_INFO.indexOf(item) > -1
      });
      var notify = 'false' == true;
      var verify = 'false' == true;
      new Valine({
          el: '.vcomment',
          notify: notify,
          verify: verify,
          appId: "vLPzzKBu42xSePsXQ9SMFMnu-gzGzoHsz",
          appKey: "SXT5mro1fOk42Qd9iF4lqApa",
          placeholder: "Just go go",
          pageSize:'10',
          avatar:'mm',
          lang:'zh-cn'
      });
    </script>
  

  

  

  

  

  
  





</body>
</html>